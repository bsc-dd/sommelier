{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sommelier**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"3\"> \n",
    "    \n",
    "***We are thrilled to present a new approach to retrieve a small piece of significant data: the Sommelier Sampling.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **INTRODUCTION:**\n",
    "\n",
    " \n",
    " <font size=\"3\"> \n",
    "Before we get hands on with the development, some technical background is needed. With the raise of Big Data and Analytics, the demand for optimization on the way data is handled is growing every day. Sampling is a powerful but also feared technique for approximating query answers, since the error estimation is an open issue for the community. Luckily, experts take care of writing the reports that demonstrate the usability of it, along with rules and calculations of the expected error.  \n",
    "<br/>\n",
    "<br/>\n",
    "More specifically, the background is separated in two blocks: the Microsoft and the BlinkDB solutions, both of them at the state of the art in query sampling. Moreover, we also mention the research that has already been made for the topic in Spark, although so far, we have only found implementations using RDD and nothing open-sourced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## QuickR: Lazily Approximating Complex AdHoc Queries in Big Data Clusters \n",
    "\n",
    "\n",
    " <font size=\"3\"> \n",
    "    \n",
    "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/quickr-2.pdf \n",
    "\n",
    "In the year 2016, Microsoft presents a system called QuickR that approximates the answer to complex queries by injecting samplers on the fly and without requiring pre-sampled data. Through a cost-based optimizer, they generate query plans with appropriate sample operators at the most adequate location, providing also an accuracy analysis to not miss any little group. The results had shown that QuickR improves substantially the performance in a large segment of TPC-DS benchmark queries, using a 2x fewer resources in the cluster and with a mean error of only the 10% in aggregations.  \n",
    "\n",
    "How did they accomplish that? \n",
    "\n",
    "By analyzing the queries, the user proposes and taking into account the statistics of the tables, they can check whether is better to extract a part of the information and which technique should they execute to offer the most accurate answer. They had three different types of sampling (which we will summarize later with examples): \n",
    "\n",
    "- Uniform \n",
    "\n",
    "- Universe \n",
    "\n",
    "- Distinct \n",
    "\n",
    "They also implement some logical optimization rules for passing the operator through projection and join  (left, right, both or none of them)  to retrieve less data in early stages of the query, and execute an accuracy analysis of the result to satisfy the requirements of the user.  \n",
    "\n",
    "An analysis of approximability of big data queries shows: \n",
    "\n",
    "1. Distribution of queries over input datasets is heavy-tailed. Individual queries use many columns and a diverse set of columns such that the additional storage space required to store samples can be prohibitively large. \n",
    "\n",
    "2. Queries typically have aggregation operators, large support, and output << input, so they are approximable. \n",
    "\n",
    "3. Several factors hinder approximability: queries use a diverse set of columns requiring extensive stratification. Many queries join large input relations. \n",
    "\n",
    "4. Queries are deep, involving multiple effective passes over data including nextwork shuffles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Experience with Approximating Queries in Microsoft’s Production Big-Data Clusters \n",
    "\n",
    " <font size=\"3\"> \n",
    "    \n",
    "    \n",
    "https://www.microsoft.com/en-us/research/uploads/prod/2019/04/p698-kandula.pdf  \n",
    "\n",
    "Three years later that the previous paper talking about the QuickR solution, the error approximation and pushdown rules, they published their overall experience (over tens of production clusters) on using this type of query time sampling. Just to remember, the two main steps of this technique are:  \n",
    "\n",
    "1. Users add sampling operators on the query \n",
    "\n",
    "2. The Query Optimizer transforms the predicate pushing down the operators while ensuring the accuracy stills intact  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"3\"> \n",
    "The information they found is valuable because it allows us to confirm that this solution is useful in production environments. Here we provide a resume of the conclusions, but you can compare the charts and results in the link of the paper.  \n",
    "\n",
    "- All three samplers are used evenly \n",
    "\n",
    "- One third of the queries use multiple sample operators.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/microsoft.png\" width=\"400\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"3\"> \n",
    "\n",
    "- The median processing rate for samplers is over 100MBps. Over 95% of samplers process input at over 100Mbps (low processing rate means very little data!) \n",
    "\n",
    "- A significant majority of the samplers doesn’t have memory footprint \n",
    "\n",
    "- Distinct sampler has more complex implementation (it had to track groups that had a large number of rows) \n",
    "\n",
    "- About 30% of the samplers picks less than 1% fraction of input. Many samplers also pick 10% (this was the recommended setting of the manual they provided)  \n",
    "\n",
    "- Fewer than 10% of the samplers use a probability assignment above 0.25 \n",
    "\n",
    "- 80% of universe samples are over a single column \n",
    "\n",
    "- The 90% of the input column sets of distinct is between 1 and 6 \n",
    "\n",
    "- Among the jobs that use samplers, only 2% never had to be re-executed. And over the 80% repeat at least 100x each.  \n",
    "\n",
    "- The results on the TPC-H Benchmark showed that: \n",
    "\n",
    "- Roughly 8 of 22 queries are unsampled \n",
    "\n",
    "- Queries 5, 7, 8 and 9 improve substantially their processing cost  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## BlinkDB \n",
    " <font size=\"3\"> \n",
    "    \n",
    "https://sameeragarwal.github.io/blinkdb_eurosys13.pdf \n",
    "\n",
    "BlinkDB proposes and implements an approximate query engine for running interactive SQL queries on large volumes of data. It supports ad-hoc queries with error and response time constraints. \n",
    "\n",
    "BlinkDB implements a multi-dimensional sampling strategy that builds and maitnains a variety of samples. Also, it implements a run-time dynamic sample selection strategy that uses parts of a sample to estimate query selectivity and chooses the best samples for satisfying query constraints.  \n",
    "\n",
    "It handles a variety of queries with diverse error and time constraints. 2s on 17 TB of data with 90-98% accuracy. \n",
    "\n",
    "</font>\n",
    "\n",
    "<img src=\"images/blinkdb.png\" width=\"400\" height=\"600\">\n",
    "<center>BlinkDB’s Implementation Stack </center>\n",
    "<br/>\n",
    "<br/>\n",
    " <font size=\"3\"> \n",
    "\n",
    "\n",
    "To store differently stratified samples, a-priori storage techniques typically use sample storage of 1x to 10x the size of the input. We’ve seen in a benchmark that the smallest input set used by 20% of queries is 3PB. Hence, assuming input popularity can be predicted perfectly, covering 20% of queries will require between 3PB and 30PB of a-priori samples. Such a large sample set is already a substantial fraction of the total input size (120PB).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# **UNDERSTANDING DATA**:\n",
    "\n",
    " <font size=\"3\"> \n",
    "In this notebook we present a few examples to better understand the different types of samples that exist and when they should be used. <br> We create synthetic datasets, normally distributed data and skewed data affects query results when sample techniques are used. We will see how the cardinality between data is key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sampling types\n",
    "\n",
    " <font size=\"3\"> \n",
    "The sampling types used for a query will vary depending on the data, data with normal distribution allows to use uniform sample. Skewed data, usually means that we need to `groupBy` the data so that we have at least some rows from every group in the data. We will use universe sample when two tables have the same size and are “row aligned”. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"3\"> \n",
    "Imagine we have two tables on our system: <br/>\n",
    "<br/>\n",
    "\n",
    "```sql \n",
    "    CREATE TABLE user ( \n",
    "        user_id LONG,  \n",
    "        city_id LONG \n",
    "    ) \n",
    "    PRIMARY KEY user_id \n",
    "```\n",
    "<br/>\n",
    "    \n",
    "```sql\n",
    "    CREATE TABLE cities ( \n",
    "        city_id LONG, \n",
    "        name TEXT, \n",
    "        cash INT\n",
    "    ) \n",
    "    PRIMARY KEY city_id \n",
    "```\n",
    "  \n",
    "<br/>\n",
    "    \n",
    "User_id and city_id are the identifiers for both tables, while Cash means the salary for that city. In our test we will do a join operation followed by a sum of the money, grouping by cities to show the differences among distributions and why is best to use one sample or another depending on the case. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.192.235.138:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1582552171248)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defined class User\n",
       "defined class Cities\n",
       "randomizeCity: Long\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class User(user_id: Int, city_id: Long)\n",
    "case class Cities(city_id: Long, name: String, cash: Int = 100)\n",
    "def randomizeCity= scala.util.Random.nextInt(5).toLong +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## **Uniform sample**\n",
    "\n",
    " <font size=\"3\"> \n",
    "Also known as Simple Random Sampling(SRS), it is assumed that the population is independent and identically distributed (i.i.d). The sample size required to reach a prespecified precision is based on the dispersion variance of the population and the survey precision required. Then the sample units are chosen from the population independently with equal probability, and inferences are conducted using the sample. I.e. We will use this kind of sample when every row from a table has the same probability of being selected. \n",
    "</font>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "<img src=\"images/uniform-sample.png\" width=\"400\" height=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1 to 100000).map(x => User(x, randomizeCity)).toDS.createOrReplaceTempView(\"users\")\n",
    "(1 to 5).map(x => Cities(x, \"a\")).toDS.createOrReplaceTempView(\"cities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query: org.apache.spark.sql.DataFrame = [city_id: bigint, res: bigint]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query = spark.sql(s\"\"\"\n",
    "                SELECT users.city_id, \n",
    "                       sum(cities.cash) AS res\n",
    "                FROM users \n",
    "                JOIN cities \n",
    "                    ON users.city_id == cities.city_id\n",
    "                GROUP BY users.city_id\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uniform_sample: org.apache.spark.sql.DataFrame = [city_id: bigint, res_sample: bigint]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val uniform_sample =  spark.sql(s\"\"\"\n",
    "                SELECT users.city_id, \n",
    "                       sum(cities.cash*100) AS res_sample\n",
    "                FROM users TABLESAMPLE(1 PERCENT) \n",
    "                JOIN cities \n",
    "                    ON users.city_id == cities.city_id\n",
    "                GROUP BY users.city_id\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|city_id|    res|\n",
      "+-------+-------+\n",
      "|      5|1994000|\n",
      "|      1|2016700|\n",
      "|      3|1979600|\n",
      "|      2|1986000|\n",
      "|      4|2023700|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|city_id|res_sample|\n",
      "+-------+----------+\n",
      "|      5|   1850000|\n",
      "|      1|   2090000|\n",
      "|      3|   2330000|\n",
      "|      2|   1870000|\n",
      "|      4|   1980000|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uniform_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.createOrReplaceTempView(\"query\")\n",
    "uniform_sample.createOrReplaceTempView(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "error: org.apache.spark.sql.DataFrame = [city_id: bigint, error: double]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val error = spark.sql(s\"\"\"\n",
    "            SELECT query.city_id, \n",
    "                   abs(res - res_sample)/res*100 AS error \n",
    "            FROM query \n",
    "            JOIN sample \n",
    "                ON query.city_id == sample.city_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|city_id|            error|\n",
      "+-------+-----------------+\n",
      "|      5|7.221664994984955|\n",
      "|      1|3.634650666931125|\n",
      "|      3|17.70054556476056|\n",
      "|      2|5.840886203423968|\n",
      "|      4|2.159410979888323|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "error.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## **Distinct sample**\n",
    "\n",
    " <font size=\"3\"> \n",
    "The uniform sample is simple but it has some issues that limit it from being used widely. Queries with group-by such as `SELECT X, SUM(Y) GROUP BY X` can miss groups in the answer, especially those corresponding to values of X that have low support. For such queries, we must use a different kind of sample, such as the distinct sampler which intuitively guarantees that at least a certain number of rows pass per distinct combination of values of a given column set. The distinct sample also helps when aggregates have high skew. When we have skewed data, few rows can contain high values (e.g revenue of a company) in a way that, for a given query that aggregates such values, these rows are crucial to obtain approximate results and hence they should have a higher probability of being selected. \n",
    "</font>\n",
    "<br/>\n",
    "<br/>\n",
    "    \n",
    "    \n",
    "<img src=\"images/distinct-sample.png\" width=\"400\" height=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users_data_1: org.apache.spark.sql.Dataset[User] = [user_id: int, city_id: bigint]\n",
       "users_data_2: org.apache.spark.sql.Dataset[User] = [user_id: int, city_id: bigint]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val users_data_1 = (1 to 300).map(x => User(x, randomizeCity)).toDS\n",
    "val users_data_2 = (301 to 100000).map(x => User(x, 5)).toDS\n",
    "\n",
    "users_data_1.union(users_data_2).createOrReplaceTempView(\"users\")\n",
    "(1 to 6).map(x => Cities(x, \"a\")).toDS.createOrReplaceTempView(\"cities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query_skewed: org.apache.spark.sql.DataFrame = [city_id: bigint, res: bigint]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query_skewed = spark.sql(s\"\"\"\n",
    "                SELECT users.city_id, \n",
    "                       sum(cities.cash) AS res\n",
    "                FROM users \n",
    "                JOIN cities \n",
    "                    ON users.city_id == cities.city_id\n",
    "                GROUP BY users.city_id\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_skewed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> \n",
    " <font size=\"4\"> \n",
    "    - What happens if we try to perform a uniform sample on users table now that the table is <b>skewed</b>?\n",
    "</font>\n",
    "<br/> \n",
    "<br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|city_id|    res|\n",
      "+-------+-------+\n",
      "|      5|9976800|\n",
      "|      1|   5600|\n",
      "|      3|   6500|\n",
      "|      2|   5900|\n",
      "|      4|   5200|\n",
      "+-------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "uniform_sample_skewed: org.apache.spark.sql.DataFrame = [city_id: bigint, res_sample: bigint]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val uniform_sample_skewed =  spark.sql(s\"\"\"\n",
    "                SELECT users.city_id, \n",
    "                       sum(cities.cash*100) AS res_sample\n",
    "                FROM users TABLESAMPLE(1 PERCENT) \n",
    "                JOIN cities \n",
    "                    ON users.city_id == cities.city_id\n",
    "                GROUP BY users.city_id\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|city_id|res_sample|\n",
      "+-------+----------+\n",
      "|      5|   9830000|\n",
      "|      3|     10000|\n",
      "|      4|     10000|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uniform_sample_skewed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> \n",
    " <font size=\"4\"> \n",
    "    - Some groups in the data are <b>missing</b>! Moreover, we obtain <b>high error</b> in the output result.\n",
    "</font>\n",
    "<br/> \n",
    "<br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_skewed.createOrReplaceTempView(\"query\")\n",
    "uniform_sample_skewed.createOrReplaceTempView(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "error: org.apache.spark.sql.DataFrame = [city_id: bigint, error: double]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val error = spark.sql(s\"\"\"\n",
    "            SELECT query.city_id, \n",
    "                   abs(res - res_sample)/res*100 AS error \n",
    "            FROM query \n",
    "            JOIN sample \n",
    "                ON query.city_id == sample.city_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|city_id|             error|\n",
      "+-------+------------------+\n",
      "|      5|1.4714136797369899|\n",
      "|      3| 53.84615384615385|\n",
      "|      4|  92.3076923076923|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "error.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> \n",
    " <font size=\"4\"> \n",
    "    - Now we are going to take into account the groups in the data, we <b>groupBy</b> city to take at least some rows from each group.\n",
    "</font>\n",
    "<br/> \n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stratified: org.apache.spark.sql.DataFrame = [city_id: bigint, count: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stratified = spark.sql(s\"\"\"\n",
    "                SELECT *, \n",
    "                       IF((count*0.01) >= 3,0.01, cast(1.0 as double)) AS p\n",
    "                FROM \n",
    "                (\n",
    "                    SELECT city_id, COUNT(*) AS count \n",
    "                    FROM users \n",
    "                    GROUP BY city_id\n",
    "                    \n",
    "                )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified.createOrReplaceTempView(\"stratified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users_sample: org.apache.spark.sql.DataFrame = [user_id: int, city_id: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val users_sample = spark.sql(s\"\"\"\n",
    "                SELECT user_id, users.city_id, p\n",
    "                FROM users \n",
    "                JOIN stratified \n",
    "                    ON stratified.city_id == users.city_id \n",
    "                WHERE RAND() <= p\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_sample.createOrReplaceTempView(\"users_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the sample is approximate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_size: Double = 0.01235\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sample_size = users_sample.count.toDouble/users_data_1.union(users_data_2).count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distinct_sample: org.apache.spark.sql.DataFrame = [city_id: bigint, res_sample: double]\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val distinct_sample = spark.sql(s\"\"\"\n",
    "                SELECT users_sample.city_id, \n",
    "                       sum(cities.cash / p) AS res_sample\n",
    "                FROM users_sample\n",
    "                JOIN cities \n",
    "                    ON users_sample.city_id == cities.city_id\n",
    "                GROUP BY users_sample.city_id\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|city_id|res_sample|\n",
      "+-------+----------+\n",
      "|      5|   1.003E7|\n",
      "|      1|    5600.0|\n",
      "|      3|    6500.0|\n",
      "|      2|    5900.0|\n",
      "|      4|    5200.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinct_sample.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> \n",
    " <font size=\"4\"> \n",
    "    - As you can see, we have now obtained a very <b>good result</b>. All the groups are represented and the output error is conceivable.\n",
    "</font>\n",
    "<br/> \n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_sample.createOrReplaceTempView(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "error: org.apache.spark.sql.DataFrame = [city_id: bigint, error: double]\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val error = spark.sql(s\"\"\"\n",
    "            SELECT query.city_id, \n",
    "                   abs(res - res_sample)/res*100 AS error \n",
    "            FROM query \n",
    "            JOIN sample \n",
    "                ON query.city_id == sample.city_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|city_id|             error|\n",
      "+-------+------------------+\n",
      "|      5|0.5332371100954214|\n",
      "|      1|               0.0|\n",
      "|      3|               0.0|\n",
      "|      2|               0.0|\n",
      "|      4|               0.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "error.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## **Universe sample**\n",
    "\n",
    "<font size=\"3\"> \n",
    "When two large tables are joined with a shared key, uniform sampling both the join inputs is not useful. Distinct sampling both the inputs has limited gains if the join keys have many columns and hence, many distinct values. Universe sample allows to sample the inputs of joins. It picks a p fraction of the values of the columns in a set (two tables of the same size). E.g. select all rows with module 500. \n",
    "\n",
    "</font> \n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<img src=\"images/universe-sample.png\" width=\"400\" height=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(1 to 500000).map(x => User(x, x)).toDF().createOrReplaceTempView(\"users\")\n",
    "(1 to 500000).map(x => Cities(x, \"a\")).toDF().createOrReplaceTempView(\"cities\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query: org.apache.spark.sql.DataFrame = [city_id: bigint, res: bigint]\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query = spark.sql(s\"\"\"\n",
    "                    SELECT users.city_id, sum(cities.cash) AS res\n",
    "                    FROM users \n",
    "                    JOIN cities \n",
    "                        ON users.city_id == cities.city_id\n",
    "                    GROUP BY users.city_id\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampling_after_join: org.apache.spark.sql.DataFrame = [city_id: bigint, res_sample: bigint]\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "val sampling_after_join = spark.sql(s\"\"\"\n",
    "                    SELECT city_id,\n",
    "                           sum(cash) AS res_sample\n",
    "                    FROM (\n",
    "                        SELECT users.city_id, cities.cash \n",
    "                        FROM users \n",
    "                        JOIN cities \n",
    "                            ON users.city_id == cities.city_id\n",
    "                    ) TABLESAMPLE (1 PERCENT)\n",
    "                    GROUP BY city_id\n",
    "                    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|city_id|res|\n",
      "+-------+---+\n",
      "|     26|100|\n",
      "|     29|100|\n",
      "|    474|100|\n",
      "|    964|100|\n",
      "|   1677|100|\n",
      "+-------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "Time: 10.25024151802063 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|city_id|res_sample|\n",
      "+-------+----------+\n",
      "|  17043|       100|\n",
      "|  29824|       100|\n",
      "|  37261|       100|\n",
      "|  38510|       100|\n",
      "|  75411|       100|\n",
      "+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Time: 9.633416175842285 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sampling_after_join.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users_sample: org.apache.spark.sql.DataFrame = [user_id: int, city_id: bigint]\n",
       "cities_sample: org.apache.spark.sql.DataFrame = [city_id: bigint, name: string ... 1 more field]\n",
       "universe_sample: org.apache.spark.sql.DataFrame = [city_id: bigint, res_sample: bigint]\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val users_sample = spark.sql(s\"\"\"SELECT * FROM users WHERE city_id % 500 == 0\"\"\")\n",
    "users_sample.createOrReplaceTempView(\"users_sample\")\n",
    "\n",
    "val cities_sample = spark.sql(s\"\"\"SELECT * FROM cities WHERE city_id % 500 == 0\"\"\")\n",
    "cities_sample.createOrReplaceTempView(\"cities_sample\")\n",
    "\n",
    "\n",
    "val universe_sample = spark.sql(s\"\"\"\n",
    "                    SELECT users_sample.city_id,\n",
    "                           sum(cities_sample.cash) AS res_sample\n",
    "                    FROM users_sample \n",
    "                    JOIN cities_sample \n",
    "                        ON users_sample.city_id == cities_sample.city_id\n",
    "                    GROUP BY users_sample.city_id\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|city_id|res_sample|\n",
      "+-------+----------+\n",
      "| 265000|       100|\n",
      "| 294500|       100|\n",
      "| 350000|       100|\n",
      "| 372000|       100|\n",
      "| 419000|       100|\n",
      "+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Time: 1.7970502376556396 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "universe_sample.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "As you can see, there's a notable difference in the time of processing the join on one query and the other, and the same result is obtained (since the cash is the same on every user, but also if the column follows a normal distribution, it wouldn't alterate significantly the result). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# **PROPOSAL:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"3\"> \n",
    "\n",
    "Right now, the spark sample implementation does not allow the logical operator to be pushed down in the Logical Plan Tree. The next Figure shows a simple Logical Plan Tree: \n",
    "</font>\n",
    "<img src=\"images/wopushdown.png\" width=\"300\" height=\"600\">\n",
    " \n",
    " <font size=\"3\"> \n",
    "We want to integrate push down for the sampler operator. In this case the sampler would be performed at the largest table (user). \n",
    "</font>\n",
    " \n",
    "<img src=\"images/pushdowned.png\" width=\"300\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"3\"> \n",
    "   \n",
    "So far, we are discussing two possible implementations. We can directly implement the Microsoft approach, but we are considering a different approach that makes it easier for the user. We are calling it Sommelier. \n",
    "    \n",
    "    \n",
    "- The Microsoft way \n",
    "\n",
    "The user needs to take into account the weight of the rows when the sample is performed, if the query involves a count or a sum operation, we need to weight the result according to the sample precision. \n",
    "<br/>\n",
    "    <br/>\n",
    "```sql\n",
    "    SELECT SUM(cities.cash)*weight\n",
    "    FROM ( \n",
    "        SELECT * \n",
    "        FROM user, cities \n",
    "        WHERE user.city_id == cities.city_id \n",
    "    ) \n",
    "    TABLESAMPLE (1% TOLERANCE) as weight  \n",
    "    GROUP BY (cities.city_id) \n",
    "    // Could also be TABLESAMPLE (1% PERCENT) as weight \n",
    "```\n",
    "   <br/>\n",
    "\n",
    "- Sommelier \n",
    "\n",
    "We want to make it easier for the user, in the Sommelier approach the user only needs to add the max error that can be allowed when using samples in the whole query. \n",
    "\n",
    "```sql\n",
    "    SELECT SUM(cities.cash)\n",
    "    FROM ( \n",
    "        SELECT * \n",
    "        FROM user, cities\n",
    "        WHERE user.city_id == cities.city_id \n",
    "    )  \n",
    "    GROUP BY (c.city_id) \n",
    "    TABLESAMPLE (1% TOLERANCE) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# **OVERALL SCHEMA:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"3\"> \n",
    "Since this project is aiming to modify and/or extend the current implementation of Spark SQL, we want to provide an intuition on the schema.  Our first intention is to develop outside the spark code, since SparkSessionExtension exists. This particular class was launched in Spark 2.2, it is pluggable and extensible and his basic function is let the user add customized extensions to the Catalyst Query Optimizer. The supported customizations include: \n",
    "\n",
    "- Custom Parser \n",
    "\n",
    "- Custom Analysis Rules (Analyzer Rules and Check Analysis Rules) \n",
    "\n",
    "- Custom Logical Rules (Optimizer Rules) \n",
    "\n",
    "- Custom Spark Strategies (Planning Strategies) \n",
    "\n",
    "- External Catalog Listeners \n",
    "    \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/spark-custom.png\" width=\"700\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"3\"> \n",
    "We propose an extension of the following components: \n",
    "\n",
    "1. **Parser.**\n",
    "\n",
    "First things first, we need to include some changes in the predicates to let the user try our new way of sampling. As seen in latest sections, this can appear as a new column w (which maybe doesn’t really need to be detected in the parser, but as a Catalyst Expression), or changing the TABLESAMPLE operator.  \n",
    "\n",
    "Currently, the last one only supports ROWS and PERCENT as arguments, so the intention is to include a TOLERANCE clause. The detection of the new attribute is a must for triggering the subsequent processes. \n",
    "\n",
    "2. **New logical operators.**\n",
    "\n",
    "In order to define different types of sampling, we need to include them in the Logical Plan to become more efficient detecting and optimizing them. The uniform one is already present on the tree as the basic Sample operator, which selects rows following a random distribution.  \n",
    "\n",
    "Spark would choose between the Universe Sample and Distinct Sample  depending on the characteristics of the tables below and the type of the query the user specifies (use Distinct if they follow a more skewed distribution...) \n",
    "\n",
    "3. **Logical Optimization Rules.**\n",
    "\n",
    "Some rules for the above new operators would improve the performance of the sampling by pushing down the sample operator in later stages of the query. We have to be able to detect the presence of Distinct, Universe and Uniform samples and have control over the Logical Plan tree.  \n",
    "\n",
    "4. **Error Estimation**\n",
    "\n",
    "Before each step of the whole process, we have to check that the user tolerance condition is maintained intact, meaning that a set of stats about the estimated error should be calculated. This would include attributes like selectivity, cardinality and so on.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/modification.png\" width=\"700\" height=\"600\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
